{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnShLYf/sshOJNZv2R2WF5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElenaManari/MLPNS_EManari/blob/main/Appunti_4_maggio_Elena.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT3 and society \n",
        "Transformer, classe di modelli che utilizzano il concetto di attenzione per predirre risultati su time series analysis. \n",
        "Creare chat bot che possono dare una risposta se interrogati che è coerente. Il goal del chat bot, GPT3, bandito in italia, ma utilizzato nei paesi in cui è consentito l'utilizzo per i task di scrittura. \n",
        "I chat box possono produrre risposta coerente dal punto di vista linguistico ma non corretta dal punto di vista del contenuto. In generale i modelli che generano sentences si chiamano language model. \n",
        "Analisi del lingueggio : natural language progessing NLP. Deep learning model. Transormers. Quello che si è utilizzato con l'utilizzo dei transformer : modelli deep learning con un numero di parametri dell'ordine dei milioni. Per trainarli li trainano su selected internet content. Alcuni su redit, o altre piattaforme. Train set viene da internet. Il linguaggio impara la struttura sintatticxa ma anche il contenuto. \n",
        "La performance migliora nella coerenza del linguaggio ma il contenuto può essere scorretto e pericoloso. \n",
        "Contenuto molto brutto fatto su training data bayas. \n",
        "Nella traduzione, lo stesso societal byas.. tradurre un linguaggio non gender speific in uno gender specific. Il sesso del soggetto è dato dalla base del contenuto. Eredita il byas sui gender roles. \n",
        "Sono indesiderati per :\n",
        "1) Il contenuto è reinserito nelle librerie di internet. Feedback loop. Il programma impara dai byas, riproduce il byasd e lo riproduce. \n",
        "2) Non c'è modo di assicurarsi se il contenuto è di un chat box o di altra rpovenienza. Ma i chat box non hanno contenuto sicuro. Ma il contenuto sembrta convincente. \n",
        "# Phylosopher : etica dell' AI \n",
        "1) leggo paper sui transfornmer : analisi etica, per parlarne sull'esame \n",
        "2) tecnico sui transformer : paper tecnico su come funziona il transformer\n",
        "On the danger of stochastic (basati su random/probabilità) parrots ( pappagallo = parlano)\n",
        "\n",
        "Punti salienti: costo dal punto di vista dell'enviroment. Quantità alte di parametri. Molto tempo per trainarli, risorse energetiche e tempo. Costyo non insignificativo, impatto di CO2 volando con gli aerei ecc ecc . Il costo ambientale è alto e viene pagato da popolazioni che sono più in pericolo, che hanno meno risorse per far fronte all'impatto ai disastri ambientali. Ma loro non usufruiscono di queste cose. Non hanno benefici da questi modelli. \n",
        "Costi finanziari\n",
        "Opportunity cost: risorse che non vengono utilizzate per altri tipi di analisi\n",
        "Rischio su individui per ripetizionbe di stereotipy. Facial recognition model ecc ecc . \n",
        "# Altri modelli per il time serties analysis\n",
        "Vanishing gradient. Architetture più semplici e storiche: Feed forward NN . Hidden layer, neuroni ecc ecc. \n",
        "Architettura che non si presta alla time series analysis e non si presta alle analisi delle immagini. Autoencoder. Si perde lì'immagine. I multi layer perceptron imparano le relazioni fra diversew features ma non ritengono info sull'ordine della time series. A noi interessa la relazione fra un punto ed un altro quando passa il tempo. \n",
        "\n",
        "Allora creo struttura che predice il futuro della time series . Time series forcasting: connessioni ricorrenti che connettono input a diversi time stamp. \n",
        "Il più semplice RNN è della forma della slides. \n",
        "Ciò funzionba bene per la separazione di untime stamps. Per più di un time stamps: input layer  ecc ecc .. Connessione per avere una predizione più completa. \n",
        "La connessione  vine persa più time stamps di separazione ho. Perchè faccio back propagation . se traino il NN, faccio la derivata dela derivata della derivata ecc ecc.. sempre più piccole. Perdo l'info più vado indietro. \n",
        "I layer vicini al time stamps imparano molto velocemento. Quelli indietro meno rapidamente fino a non imparare per nulla e perdo l'info che mi davano. \n",
        "Problema complesso: quando ho la struttura ricorrente perdo info dei time stamp più indietro. La predizione si basa sulle due tre parole prima di quella da tradurre e non su tutto il senso della frase. Porta a predizioni vulnerabili. \n",
        "Non è in programma questa parte . \n",
        "Ricurrent NN . \n",
        "# LSTM long short tern memory \n",
        "Prendo non tutti i link ma solo alcuni link. Concvetto di gate. (slides che non chiede)\n",
        "\n",
        "# Meccanismo di attenzione \n",
        "Matrice, covariance matrix tra input ed input. Transormer, il paper è Attention is all you need . Concetto di attenzione precede il paper di pochi anni. \n",
        "Ha un encoder e decoder, come tipico dei modelli generativi. Latent space e da li il decoder produce nuovi dati. \n",
        "Entrambi hanno meccanismi di attenzione, pesano gli input in modo diverso. Serie di misuramenti e parole. Valutano l'importanza che ci sia una parola o no nel contesto della frase. \n",
        "Meccanismo è il multi headed attention, matrice covariante + altre matrici che imparano diversi pesi, pezzi della frase si collegano ad altri pezzi della frase in maniera complessa. Pesi imparati trainando il modello. Ciascuna delle variabili / features è una parola. Voglio impoarare la relazione tra il fatto che c'è parola gatto, verbo ecc ecc. Contenuto strutturale della frase. Verbo plurale ecc ecc . Relazioni misurate dai pesi della matrice. \n",
        "# Word tockenization \n",
        "Il linguaggio usa parole. Il computer usa numeri. \n",
        "Processo per trasformare le parole in numeri. Tokenizzazione = come trasformo la parola in un vettore, non un numero . \n",
        "Vettore = se parola espressa come vettore posso vedere relazione fra due vettori. spazio più complesso che tra due numeri. Ad ogni parola corrispondono dizionari specifici. Vector embedding. Siccome sono vettori: similitudine con il dot prodot , perpendicolari o paralleli. Non tipicamente associate  = perp, tipicamente associate = paralleli. Più l'angolo è piccolo più le parole sono relazionate. \n",
        "\n",
        "Tokckenize language\n",
        "# Attenzione\n",
        "faccio la stessa cosa su una matrice più grande. \n",
        "Pesi probabilistici. Introdotto nel 2015. Single attention mechanism per il problema di Vanishing, il LSTM lo aveva infatti risolto solo parzialmente. \n",
        "Il transormer è multi headed attention. Fornisco più di una matrice. per avere relazione più complessa tra elementi del linguaggio . \n",
        "# key - value- query \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ALpXfpv-GNMH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "alPmct2UKB8Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}